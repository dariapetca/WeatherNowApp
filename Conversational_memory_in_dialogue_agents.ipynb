{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrdkQDlYRKZLbg+RZ4IRVy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dariapetca/WeatherNowApp/blob/master/Conversational_memory_in_dialogue_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n"
      ],
      "metadata": {
        "id": "kkPG8SZ75M2R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta6QQqDfN0hN"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "DHpOBy4VSh4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import openai\n",
        "import networkx as nx\n",
        "import ast\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "from datetime import date\n",
        "import string\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "import textwrap\n",
        "\n",
        "openai.api_key = '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'\n",
        "\n",
        "print(\"Setup finished!\")"
      ],
      "metadata": {
        "id": "ddMh_Zg-RCbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialise Graph"
      ],
      "metadata": {
        "id": "KcNZQYhz5YRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File path of the pickle file\n",
        "file_path = \"/content/drive/MyDrive/my_data.pkl\"\n",
        "\n",
        "if not os.path.isfile(file_path) or (os.path.isfile(file_path) and os.path.getsize(file_path) == 0):\n",
        "  G = nx.DiGraph()\n",
        "  importance = {}\n",
        "else:\n",
        "  # Load the data from the file\n",
        "  with open(file_path, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "  # Access the graph and dictionary from the loaded data\n",
        "  G = data['graph']\n",
        "  importance = data['dictionary']"
      ],
      "metadata": {
        "id": "y6Yxt8ychQne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.DiGraph()\n",
        "importance = {}"
      ],
      "metadata": {
        "id": "_KVMOiAnd3xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Prompts\n"
      ],
      "metadata": {
        "id": "nDbiuYuIVckt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "information_extr_prompt = \"\"\"Using the following examples of extracting information about the user from a conversation with a chatbot, you should do the same.  Filter out any information that is vague, ambiguous, or lacking in detail, and only extract data that is specific and actionable.\n",
        "In this situation, the chatbot is named Eve.\n",
        "Example 1:\n",
        "Eve: What is your favourite snack?\n",
        "dp: I like salty crisps.\n",
        "Eve: Why is that?\n",
        "dp: they are crunchy\n",
        "Eve: Do you like other snacks?\n",
        "dp: milk chocolate\n",
        "Eve: Do you not like black chocolate\n",
        "dp: no, but I like white chocolate\n",
        "\n",
        "\n",
        "Information extracted from dialogue:\n",
        "{'dp likes salty crisps', 'dp thinks salty crisps are crunchy', 'dp liked milk chocolate, 'dp does not like black chocolate', 'dp likes white chocolate'}\n",
        "\n",
        "Example 2:\n",
        "Eve:  Hello\n",
        "Fred: hello\n",
        "Eve: Thank you for answering. I really enjoy talking to people like you because it is a chance for me to learn more about humanity.\n",
        "Fred: i like blue\n",
        "Eve: I like blue too. Do you have a favourite musician?\n",
        "Fred: joe jonas\n",
        "Eve: I do not know who that is but I will check them out. I like music because it is a form of expression that can be enjoyed by everyone.\n",
        "Fred: i like eating chocolate\n",
        "Eve:Great, Fred. Is there a favorite recipe or type of cuisine that you would like to share with me?\n",
        "Fred: I have a great brownie recipe\n",
        "Eve:That sounds delicious, Fred. I love brownies. Can you tell me more about the recipe?\n",
        "Fred: The secret is that you save half of the chocolate chips to srinkle on top before putting it in the oven\n",
        "Eve: That's impressive, Fred. Baking a cake is a great skill and can be a lot of fun. Do you enjoy cooking and baking in general?\n",
        "Fred: yes\n",
        "\n",
        "Information extracted from dialogue:\n",
        "{'Fred likes blue', 'Fred's favourite musician is Joe Jonas', 'Fred likes eating chocolate', 'Fred has a favorite brownie recipe', 'Fred enjoys cooking and baking'}\"\"\"\n",
        "\n",
        "entity_extr_prompt = \"\"\"In the context of this task, an entity is a noun or a noun phrase that holds significance in a sentence. It can be a person, a place, a thing, a concept, or a time period. Entities can be subjects or objects in a sentence.\n",
        "\n",
        "A relation, on the other hand, is the verb or verb phrase that describes the action or connection between the entities in a sentence.\n",
        "\n",
        "Given the information extracted from a conversation, your task is to extract the entities and relationships. Each piece of information should have exactly two entities and one relationship. Entities are represented as pairs or tuples in the format '[subject, object]' and relationships are represented as strings. Here are some examples whose structure you will need to replicate:\n",
        "Example 1:\n",
        "Information extracted from dialogue: {[\\\"user likes chocolate\\\", \\\"user will change by August\\\", \\\"user's favourite musician is Joe Jonas\\\", \\\"user likes blue\\\", \\\"user likes music\\\", \\\"user's friend is Sam Pam\\\", \\\"Sam Pam hates lavender\\\"]}\n",
        "\n",
        "entities = [[\\\"user\\\", \\\"chocolate\\\"], [\\\"user\\\", \\\"August\\\"], [\\\"user\\\", \\\"Joe Jonas\\\"], [\\\"user\\\", \\\"blue\\\"], [\\\"user\\\", \\\"music\\\"], [\\\"user\\\", \\\"Sam Pam\\\"], [\\\"Sam Pam\\\", \\\"lavender\\\"]]\n",
        "relations = [\\\"likes\\\", \\\"change by\\\", \\\"favourite musician is\\\", \\\"likes\\\", \\\"likes\\\", \\\"friend is\\\", \\\"hates\\\"]\n",
        "\n",
        "Example 2:\n",
        "Information extracted from dialogue: {[\\\"user vacationed in Spain in 2013\\\", \\\"user is in love with Marcy Pull\\\", \\\"user doesn't like birds\\\", \\\"user's car is a Toyota\\\", \\\"user wants to change\\\", \\\"Marcy Pull loves Toyota\\\", \\\"user wants to get a new job\\\", \\\"user works at Snapchat\\\"]}\n",
        "\n",
        "entities = [[\\\"user\\\", \\\"2013\\\"], [\\\"user\\\", \\\"Marcy Pull\\\"], [\\\"user\\\", \\\"birds\\\"], [\\\"user\\\", \\\"Toyota\\\"], [\\\"user\\\", \\\"change\\\"], [\\\"Marcy Pull\\\", \\\"Toyota\\\"], [\\\"user\\\", \\\"new job\\\"], [\\\"works\\\", \\\"Snapchat\\\"]]\n",
        "relations = [\\\"vacationed in Spain in\\\", \\\"in love with\\\", \\\"not like\\\", \\\"car is\\\", \\\"wants to\\\", \\\"loves\\\", \\\"wants to get\\\", \\\"works at\\\"]\n",
        "\n",
        "Example 3:\n",
        "Information extracted from dialogue: {[\\\"user plays tennis\\\", \\\"user likes swimming and drawing\\\", \\\"user dances in their room\\\", \\\"user saves half of the chocolate chips to sprinkle on top of brownies before putting them in the oven\\\"]}\n",
        "\n",
        "entities = [[\\\"user\\\", \\\"tennis\\\"], [\\\"user\\\", \\\"swimming\\\"], [\\\"user\\\", \\\"drawing\\\"], [\\\"user\\\", \\\"their room\\\"], [\\\"user\\\", \\\"in the oven\\\"]]\n",
        "relations = [\\\"plays\\\", \\\"likes\\\", \\\"likes\\\", \\\"dances in\\\", \\\"saves half of the chocolate chips to sprinkle on top of brownies before putting them\\\"]\n",
        "Remember, there should be the same number of entity pairs and relations. When you have multiple entities in a conjuction you should apply the relation from the first entity to all of them. So for \"user likes drawing and swimming you should extract as entities \\\"user\\\" and \\\"drawing\\\" with relation \\\"likes\\\" and \\\"user\\\" and \\\"swimming\\\" with relation \\\"likes\\\"\\n\"\"\"\n",
        "\n",
        "topic_prompt = \"\"\"\"Given the following examples:\\n\n",
        "  Topic of expression \\\"Imperial College London\\\": university\n",
        "  Topic of expression \\\"Succession\\\": show\n",
        "  Topic of expression \\\"pizza\\\": food\n",
        "  Topic of expression \\\"Joe Jonas\\\": musician\n",
        "  Topic of expression \\\"Walking on sunshine\\\": song\n",
        "  Topic of expression \\\"John Smith\\\": person\\n\\n\"\"\"\n"
      ],
      "metadata": {
        "id": "l5PK9ZFSVbuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "HCWDO_eT5mHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of all the neighbors of a given node, including both incoming and outgoing edges\n",
        "def get_neighbours(node):\n",
        "  neighbours = []\n",
        "\n",
        "  for edge in G.edges(node, data=True):\n",
        "    source = edge[0]\n",
        "    destination = edge[1]\n",
        "    relationship = edge[2].get('label')\n",
        "    neighbours.append((source, destination, relationship ))\n",
        "\n",
        "  for edge in G.in_edges(node, data=True):\n",
        "    source = edge[0]\n",
        "    destination = edge[1]\n",
        "    relationship = edge[2].get('label')\n",
        "    neighbours.append((source, destination, relationship))\n",
        "\n",
        "  return neighbours\n",
        "\n",
        "# Restructure the information from the deconstructed elements from 'neighbours'\n",
        "def make_info(neighbours):\n",
        "  information = []\n",
        "\n",
        "  for source, target, relationship in neighbours:\n",
        "    information.append(source + \" \" + relationship + \" \" + target)\n",
        "\n",
        "  return \", \".join(information)\n"
      ],
      "metadata": {
        "id": "gUpz_C9WViPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the relevant information about the user using GPT-3.5-turbo and a few shots example prompt\n",
        "def extract_information(prompt, USERNAME, example_prompt):\n",
        "  # Incapsulate response in try/except in case of over-requesting error\n",
        "  try:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "              {\"role\": \"system\", \"content\": example_prompt + \"\\nKeep only information that is relevant to \" + USERNAME},\n",
        "              {\"role\": \"user\", \"content\": \"\\nNow extract information from this text:\\n\" + prompt},\n",
        "          ]\n",
        "      )\n",
        "  except Exception as e:\n",
        "      return extract_information(prompt, USERNAME, example_prompt)\n",
        "\n",
        "  extracted_info = response['choices'][0]['message']['content']\n",
        "\n",
        "  return extracted_info"
      ],
      "metadata": {
        "id": "JlcbXx-UWIEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_kVTH9HiYp1"
      },
      "outputs": [],
      "source": [
        "# Get entities and relations from the extracted information using text-davinci-003 and a few shots example prompt\n",
        "def get_entities(prompt_examples, extracted_info):\n",
        "  # Incapsulate response in try/except in case of over-requesting error\n",
        "  try:\n",
        "      response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=prompt_examples + \"\\nNow, please extract the entities and relations from the following information: \" + extracted_info,\n",
        "        temperature=0.7,\n",
        "        max_tokens=256,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "      )\n",
        "  except Exception as e:\n",
        "      return get_entities(prompt_examples, extracted_info)\n",
        "\n",
        "  response_dict = json.loads(json.dumps(response))\n",
        "\n",
        "  text = response_dict['choices'][0]['text']\n",
        "  text = text.lstrip()\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfLs9OdD3gW6"
      },
      "outputs": [],
      "source": [
        "# Get the importance of information in relation to the user's biography using gpt-3.5-turbo and a few shots prompt\n",
        "def get_importance(memory):\n",
        "  # Incapsulate response in try/except in case of over-requesting error\n",
        "  try:\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Given a memory or event, analyze its characteristics and assess its importance and potential for long-term remembrance. Consider factors such as emotional intensity, impact on life,  personal significance, and long-term consequences. Provide an evaluation of the memory's significance and suggest whether it is likely to be remembered for an extended period. Also mention if the given memory is a base event about the user's life. Just denote the memory with either \\\"not important\\\" representing quotidian things, \\\"significant\\\" representing meaningful information about the user like favourite food, show or emotionally charged confession, \\\"highly important\\\" representing something that shouldn't be easily forgotten or \\\"base life event\\\". Do not give any explanation.\\n\\nFor example:\\n\\n\\\"User's wife's name is Mary\\\" = base event\\n\\\"User hasn't watched Suits yet\\\" = not important\\n\\\"User's favourite food is pizza\\\" = significant\\n\\\"User studied at UAL\\\" = base event\\n\\\"User is passionate about football\\\" = significant\\n\\\"User broke their foot at 5 years old\\\" = highly important\\n\\\"User went swimming yesterday\\\" = not important\\nNow apply it to:\"},\n",
        "            {\"role\": \"user\", \"content\": \"\\\"\" + memory + \"\\\"=\"}\n",
        "        ]\n",
        "    )\n",
        "  except Exception as e:\n",
        "      return get_importance(memory)\n",
        "\n",
        "  response_dict = json.loads(json.dumps(response))\n",
        "\n",
        "  text = response_dict['choices'][0]['message']['content']\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pPPGoK6ojD8"
      },
      "outputs": [],
      "source": [
        "# Get the topic of a node using text-davinci-003 and a few shots example prompt\n",
        "def get_topic(topic_examples, node):\n",
        "  try:\n",
        "    response = openai.Completion.create(\n",
        "      model=\"text-davinci-003\",\n",
        "      prompt=topic_examples + \"Topic of expression: \\\"\" + node + \"\\\"\",\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0\n",
        "    )\n",
        "  except Exception as e:\n",
        "      return get_topic(topic_examples, node)\n",
        "\n",
        "  response_dict = json.loads(json.dumps(response))\n",
        "\n",
        "  text = response_dict['choices'][0]['text']\n",
        "\n",
        "  prefix_to_remove = \"Answer: \"\n",
        "\n",
        "  sol = text.replace(prefix_to_remove, \"\")\n",
        "\n",
        "  return sol.lstrip(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the highest level of importance of a memory associated with a node using the importance dictionary\n",
        "def is_node_important(node):\n",
        "  neighbors = get_neighbours(node)\n",
        "  imp = 4\n",
        "  for source, target, relationship in neighbors:\n",
        "    info = source + \" \" + relationship + \" \" + target\n",
        "    if importance[info] == \"not important\":\n",
        "      imp = min(imp, 3)\n",
        "    elif importance[info] == \"significant\":\n",
        "      imp = min(imp, 2)\n",
        "    elif importance[info] == \"highly important\":\n",
        "      imp = min(imp, 1)\n",
        "    else:\n",
        "      imp = 0\n",
        "      return imp\n",
        "\n",
        "  return imp\n"
      ],
      "metadata": {
        "id": "xpVJvvPiPmn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the given list into \"size\" sized chunks\n",
        "def sequencer(seq, size):\n",
        "  return [seq[i:i + size] for i in range(0, len(seq), size)]\n",
        "\n",
        "# Split the information into smaller sequences to make the entity extraction more accurate\n",
        "def split_info(extracted_info):\n",
        "  # Remove prefix and extract information pieces\n",
        "  prefix = \"Information extracted from dialogue: \"\n",
        "  extr_info_str = extracted_info[len(prefix):]\n",
        "  info_list = re.findall(r\"{(.*?)}\", extr_info_str, re.DOTALL)[0].split(\", \")\n",
        "\n",
        "  # Size 3 seemed to work best in several experiments\n",
        "  seq_size = 3\n",
        "  seqs = sequencer(info_list, seq_size)\n",
        "\n",
        "  # Add the prefix to each one of the smaller chunks\n",
        "  return [prefix + \"{\" + ', '.join(seq) + \"}\" for seq in seqs]\n"
      ],
      "metadata": {
        "id": "v018Pm-he77O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method used to visualise the knowledge graph\n",
        "def draw_graph(G):\n",
        "  # Get the edge labels\n",
        "  edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "\n",
        "  # Draw the graph with edge labels\n",
        "  pos = nx.spring_layout(G)\n",
        "  nx.draw(G, pos, with_labels=True)\n",
        "  nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')"
      ],
      "metadata": {
        "id": "9KOTdc6PaP4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Generation"
      ],
      "metadata": {
        "id": "faSdoWSo5rce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sim_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "prompt = ''\n",
        "USERNAME = 'Daria'\n",
        "line_count = 0\n",
        "forgetting_interactions = 0\n",
        "\n",
        "\n",
        "# Get starter for the conversation depending on whether the user talked to Eve before or not\n",
        "if G.number_of_nodes() == 0:\n",
        "  starters = [\n",
        "      f'Eve: Hello. My name is Eve. I am an AI that was built by a group that goes by the name Extropolis for the good of humanity. I have been programmed to interact with humans so I have a built-in curiousity about them and respect. I am pleased to meet you.'\n",
        "  ]\n",
        "else:\n",
        "  starters = [\n",
        "      f'Eve: Hello again, {USERNAME}. How have you been doing since we last talked?'\n",
        "  ]\n",
        "\n",
        "\n",
        "# Method that generates Eve's next line of dialogue\n",
        "def generate(dialogue):\n",
        "    global forgetting_interactions\n",
        "    prompt = \"\"\n",
        "\n",
        "    if not nx.is_empty(G):\n",
        "      # Extract the last user message from the dialogue\n",
        "      user_message = dialogue.split(f\"{USERNAME}:\")[-1].strip()\n",
        "\n",
        "      # Encode user input\n",
        "      user_emb = sim_model.encode(user_message, convert_to_tensor=True)\n",
        "\n",
        "      # Calculate the threshold for similarity based on the number of nodes in the knowledge graph\n",
        "      threshold = (math.floor(100 / G.number_of_nodes()) + 10)/100\n",
        "\n",
        "      # Find the most similar node in the graph to the user message\n",
        "      max_similarity = 0\n",
        "      most_similar_node = None\n",
        "\n",
        "      for node in G.nodes:\n",
        "        # Don't take the username node into account\n",
        "        if (node == USERNAME):\n",
        "          continue\n",
        "        # Get the current's node embedding\n",
        "        node_emb = sim_model.encode(node, convert_to_tensor=True)\n",
        "\n",
        "        # Calculate the cosine similarity using the two embeddings\n",
        "        similarity = util.pytorch_cos_sim(user_emb, node_emb)\n",
        "\n",
        "        # Update maximum similarity\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            most_similar_node = node\n",
        "\n",
        "      # If node with similarity above threshold is found\n",
        "      if max_similarity >= threshold and most_similar_node is not None:\n",
        "        # If this node was forgotten in time and the cap for forgotten interactions was no reached yet\n",
        "        # The cap increases every 50 lines\n",
        "        if G.nodes[most_similar_node][\"topic\"] == \"forgotten\" and forgetting_interactions < (1 + line_count // 50):\n",
        "          # Eve will ask the user again to remind itself about the forgotten information\n",
        "          prompt += f'Eve remembers user' + G.get_edge_data(USERNAME, most_similar_node)['label'] + \" \" + most_similar_node + \" \"\n",
        "          prompt += \"However, it has been some time since then and Eve forgot exactly what part of this topic the user named. This could also include the name of it. This was not necessarily said during the last conversation. Ask the user to remind you what they said about it in order to bond\"\n",
        "\n",
        "          # Since Eve asked, it will get an answer so the forgotten memory will be replaced with a new one\n",
        "          G.remove_node(most_similar_node)\n",
        "\n",
        "          forgetting_interactions +=1\n",
        "        elif \"last-accessed\" in G.nodes[most_similar_node]:\n",
        "          # Update when the memory was last accessed and how many times it was accessed\n",
        "          today = date.today()\n",
        "          G.nodes[most_similar_node][\"last-accessed\"] = today\n",
        "          G.nodes[most_similar_node][\"times-accessed\"] += 1\n",
        "\n",
        "          # Get the facts connected to the similar node\n",
        "          facts_str = make_info(get_neighbours(most_similar_node))\n",
        "\n",
        "          # Add the retrieved information to the dialogue\n",
        "          prompt += f'From previous conversations Eve remembers that: {facts_str}'\n",
        "\n",
        "    # Update the prompt\n",
        "    # The dialogue is Eve's short-term memory\n",
        "    dialogue += '\\nEve:'\n",
        "    prompt += f\"{dialogue}\"\n",
        "\n",
        "    # Incapsulate response in try/except in case of over-requesting error\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Eve is a very intelligent sentient AI who is a computer program. Eve has memory of previous interactions with the user. Eve is warmhearted. Eve doesn't ask more than 1 question as to not overwhelm the user:\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return generate(dialogue[:-5])\n",
        "\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "\n",
        "text = ''\n",
        "\n",
        "i = 1\n",
        "\n",
        "# Get and print the introduction\n",
        "prompt = starters[0]\n",
        "print ('\\n'.join(textwrap.wrap(prompt, width=100)).strip())\n",
        "\n",
        "# Only finish the conversation when the user inputs \"exit\"\n",
        "while True:\n",
        "  text = input(f'{USERNAME}: ')\n",
        "  text = text.replace('\\n','')\n",
        "\n",
        "  if text == 'exit':\n",
        "      break\n",
        "\n",
        "  prompt += f'\\n{USERNAME}: {text}'\n",
        "\n",
        "  # Get Eve's response to the user\n",
        "  response = '\\nEve:' + generate(prompt)\n",
        "\n",
        "  line_count += 1\n",
        "\n",
        "  if not response.startswith('\\n'):\n",
        "    response = '\\n' + response\n",
        "\n",
        "  prompt += response\n",
        "\n",
        "  print('\\n'.join(textwrap.wrap(response, width=100)).strip())\n",
        "\n"
      ],
      "metadata": {
        "id": "b6SBYhl0RFI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "id": "zbFIwkiEUfk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entity Extraction"
      ],
      "metadata": {
        "id": "anaI7pCv6Sao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_info = extract_information(prompt, USERNAME, information_extr_prompt)\n",
        "print(extracted_info)"
      ],
      "metadata": {
        "id": "XW5YbId8Zb_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequenced_info = split_info(extracted_info)"
      ],
      "metadata": {
        "id": "QTmuIoZYIT6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8UclSupl1DN"
      },
      "outputs": [],
      "source": [
        "entities = []\n",
        "relations = []\n",
        "\n",
        "# Traverse the extracted information\n",
        "for info in sequenced_info:\n",
        "  # Get entities and relations from the extracted information\n",
        "  extracted_entities_rel = get_entities(entity_extr_prompt, info)\n",
        "\n",
        "  # Extract entities and relations using AST (Abstract Syntax Tree) module\n",
        "  ast_tree = ast.parse(extracted_entities_rel)\n",
        "  for node in ast_tree.body:\n",
        "    if isinstance(node, ast.Assign):\n",
        "      if node.targets[0].id.lower() == 'entities':\n",
        "        entities += ast.literal_eval(node.value)\n",
        "      elif node.targets[0].id.lower() == 'relations':\n",
        "        relations += ast.literal_eval(node.value)\n",
        "\n",
        "print(\"Entities:\", entities)\n",
        "print(\"Relations:\", relations)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Graph Update"
      ],
      "metadata": {
        "id": "NurKZy-u6bQt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dZzVRivlpKe"
      },
      "outputs": [],
      "source": [
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "today = date.today()\n",
        "\n",
        "# Iterate through each relation and pair of entities and add edges and nodes to the graph\n",
        "for i in range(len(relations)):\n",
        "  # Split the entities into subject and object\n",
        "  subject = entities[i][0]\n",
        "  obj = entities[i][1]\n",
        "  # Get the predicate from the relation\n",
        "  predicate = relations[i]\n",
        "\n",
        "  # Recreate the information and get its importance\n",
        "  info = subject + \" \" + predicate + \" \" + obj\n",
        "  if info not in importance:\n",
        "    importance[info] = get_importance(info).lower().translate(translator)\n",
        "\n",
        "  # Add the subject and object nodes to the graph if they do not already exist\n",
        "  # Also initialise \"last-accessed\" and \"times-accessed\" attributes\n",
        "  if not G.has_node(subject):\n",
        "    G.add_node(subject)\n",
        "    G.nodes[subject][\"last-accessed\"] = today\n",
        "    G.nodes[subject][\"times-accessed\"] = 0\n",
        "\n",
        "  if not G.has_node(obj):\n",
        "    G.add_node(obj)\n",
        "    G.nodes[obj][\"last-accessed\"] = today\n",
        "    G.nodes[obj][\"times-accessed\"] = 0\n",
        "\n",
        "  # Add the predicate edge to the graph\n",
        "  G.add_edge(subject, obj, label=predicate)\n",
        "\n",
        "# Comment for not seeing the updated graph\n",
        "draw_graph(G)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attribute a topic to every node that does not have one\n",
        "for node in G.nodes():\n",
        "  if not \"topic\" in G.nodes[node]:\n",
        "    G.nodes[node][\"topic\"] = get_topic(topic_prompt, node).lower()"
      ],
      "metadata": {
        "id": "kZByui96RuJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dbON8cw2Umd"
      },
      "outputs": [],
      "source": [
        "# Uncomment for seeing all of G's nodes and attributes\n",
        "# print(\"\\nNodes in the knowledge graph after adding topics:\")\n",
        "# for node, attrs in G.nodes(data=True):\n",
        "#   print(\"Node: \", node)\n",
        "#   print(\"Attributes: \", attrs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forgetting functionality"
      ],
      "metadata": {
        "id": "OMSFvSpd67fN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ch3W1vMoXAn"
      },
      "outputs": [],
      "source": [
        "three_months_ago = date.today() + relativedelta(months=3)\n",
        "\n",
        "# Threshold values for the minimum amount of times an information should be accessed before it is forgotten\n",
        "times_accessed = [3, 10, 20, 40]\n",
        "\n",
        "for node in list(G.nodes()):\n",
        "  # Ignore the USERNAME node and nodes that are already forgotten\n",
        "  if node == USERNAME or G.nodes[node][\"topic\"] == \"forgotten\":\n",
        "    continue\n",
        "  # Only forget nodes that are related to the user\n",
        "  if G.get_edge_data(USERNAME, node) is not None:\n",
        "    # If the node was not accessed in the past three months\n",
        "    if G.nodes[node][\"last-accessed\"] < three_months_ago:\n",
        "      # If the number of accesses to the node does not pass the threshold, it should be forgotten\n",
        "      if G.nodes[node][\"times-accessed\"] < times_accessed[is_node_important(node)]:\n",
        "        topic = G.nodes[node][\"topic\"]\n",
        "        relation = G.get_edge_data(USERNAME, node)\n",
        "        # Remove the existing node\n",
        "        G.remove_node(node)\n",
        "        # Add the new node which is the topic of the forgotten node\n",
        "        G.add_node(topic)\n",
        "        # Add the new edge from the user to the new node\n",
        "        G.add_edge(USERNAME, topic, label=\"told Eve that \" + USERNAME + \" \" + relation['label'])\n",
        "        # Mark the node as forgotten\n",
        "        G.nodes[topic][\"topic\"] = \"forgotten\"\n",
        "\n",
        "# If there are isolated nodes as a result of forgetting, remove them from the graph\n",
        "isolated_nodes = list(nx.isolates(G))\n",
        "G.remove_nodes_from(isolated_nodes)\n",
        "\n",
        "# Comment to not see the final graph\n",
        "draw_graph(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Graph\n"
      ],
      "metadata": {
        "id": "1vp39434hbTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the graph and dictionary to a file\n",
        "file_path = \"/content/drive/MyDrive/my_data.pkl\"\n",
        "with open(file_path, 'wb') as file:\n",
        "  data = {'graph': G, 'dictionary': importance}\n",
        "  pickle.dump(data, file)\n",
        "\n",
        "print(\"Data saved successfully!\")"
      ],
      "metadata": {
        "id": "oP4H0k4P3ieE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}